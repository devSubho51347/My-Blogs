{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2168c92f",
   "metadata": {},
   "source": [
    "## Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce85327f",
   "metadata": {},
   "source": [
    "The collection Module in Python provides different types of containers. A Container is an object that is used to store different objects and provide a way to access the contained objects and iterate over them. Some of the built-in containers are Tuple, List, Dictionary, etc. In this article, we will discuss the different containers provided by the collections module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0814250e",
   "metadata": {},
   "source": [
    "### Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab091be3",
   "metadata": {},
   "source": [
    "A counter is a sub-class of the dictionary. It is used to keep the count of the elements in an iterable in the form of an unordered dictionary where the key represents the element in the iterable and value represents the count of that element in the iterable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e67c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d240d0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter()\n",
      "Counter({'a': 3, 'l': 2, 'g': 1, 'h': 1, 'd': 1})\n",
      "Counter({'red': 4, 'blue': 2})\n",
      "Counter({'dogs': 8, 'cats': 4})\n"
     ]
    }
   ],
   "source": [
    "c = Counter()                           # a new, empty counter\n",
    "d = Counter('gallahad')                 # a new counter from an iterable\n",
    "e = Counter({'red': 4, 'blue': 2})      # a new counter from a mapping\n",
    "f = Counter(cats=4, dogs=8)             # a new counter from keyword args\n",
    "\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be389129",
   "metadata": {},
   "source": [
    "### Additional Methods Supported By Counter Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc2cc503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity 1 output:  ['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b']\n",
      "Counter 2:  Counter({6: 3, 5: 2, 1: 1, 2: 1, 3: 1, 4: 1})\n",
      "Activity 2 output:  6\n"
     ]
    }
   ],
   "source": [
    "# Method 1 :  elements()\n",
    "# Return an iterator over elements repeating each as many times as its count. \n",
    "# Elements are returned in the order first encountered. If an elementâ€™s count is less than one, elements() will ignore it.\n",
    "\n",
    "# Activity 1\n",
    "c = Counter(a=6, b=4, c=0, d=-2)\n",
    "new_li = sorted(c.elements())\n",
    "print(\"Activity 1 output: \", new_li)\n",
    "\n",
    "#Method 2: most_common([n])\n",
    "#Return a list of the n most common elements and their counts from the most common to the least. \n",
    "#If n is omitted or None, most_common() returns all elements in the counter. \n",
    "#Elements with equal counts are ordered in the order first encountered:\n",
    "\n",
    "#Activity 2\n",
    "my_list = [1,2,3,4,5,5,6,6,6,]\n",
    "counter_2 = Counter(my_list)\n",
    "print(\"Counter 2: \", counter_2)\n",
    "print(\"Activity 2 output: \", counter_2.most_common(2)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c61f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"aaabbbcccdddeeffffffffff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94698cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceffa0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'f': 10, 'a': 3, 'b': 3, 'c': 3, 'd': 3, 'e': 2})\n"
     ]
    }
   ],
   "source": [
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08a59502",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = [1,2,3,4,5,5,5,5,5,6,6,6,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1a0a904",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_2 = Counter(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcc3e9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({5: 5, 6: 3, 1: 1, 2: 1, 3: 1, 4: 1})\n"
     ]
    }
   ],
   "source": [
    "print(counter_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83cac1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# finding the most common element\n",
    "print(counter_2.most_common(2)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d697c3",
   "metadata": {},
   "source": [
    "## NamedTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90d8822",
   "metadata": {},
   "source": [
    "A NamedTuple returns a tuple object with names for each position which the ordinary tuples lack. For example, consider a tuple names student where the first element represents fname, second represents lname and the third element represents the DOB. Suppose for calling fname instead of remembering the index position you can actually call the element by using the fname argument, then it will be really easy for accessing tuples element. This functionality is provided by the NamedTuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f9f6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fa69a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Emp = namedtuple(\"Employee\",\"Name,Salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8f593da",
   "metadata": {},
   "outputs": [],
   "source": [
    "New_emp = Emp(\"Sunny\",\"30000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "435e8ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employee(Name='Sunny', Salary='30000')\n"
     ]
    }
   ],
   "source": [
    "print(New_emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16a22430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Sunny', '30000')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "New_emp[0],New_emp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "301574f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of the employee is: Sunny  and he gets around: 30000\n"
     ]
    }
   ],
   "source": [
    "#We can also print the attributes by calling their name\n",
    "print(\"Name of the employee is:\", New_emp.Name,\" and he gets around:\", New_emp.Salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a81cec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Passing list\n",
    "Point = namedtuple(\"Point\",[\"x\",\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d9de644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "New_point = Point(2,3)\n",
    "print(New_point[0],New_point[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "020ca141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Point(x=2, y=3), 2, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "New_point, New_point.x, New_point.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8268b3",
   "metadata": {},
   "source": [
    "## Chainmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37395e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChainMap({'a': 1, 'b': 2}, {'c': 3, 'd': 4}) ['c', 'd', 'a', 'b']\n"
     ]
    }
   ],
   "source": [
    "from collections import ChainMap  \n",
    "     \n",
    "     \n",
    "dict_1 = {'a': 1, 'b': 2} \n",
    "dict_2 = {'c': 3, 'd': 4} \n",
    "  \n",
    "# Defining the chainmap  \n",
    "chain = ChainMap(dict_1,dict_2)  \n",
    "     \n",
    "print(chain,list(chain))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda04cc2",
   "metadata": {},
   "source": [
    "## Deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b76e6b",
   "metadata": {},
   "source": [
    "Deque (Doubly Ended Queue) is the optimized list for quicker append and pop operations from both sides of the container. It provides O(1) time complexity for append and pop operations as compared to list with O(n) time complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6aab707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d896238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Version:  deque(['Sunny', 'Riya', 10])\n",
      "After appending elements:  deque(['Sunny', 'Riya', 10, ['20', '30']])\n",
      "deque(['Ma', 'Baba', 'Sunny', 'Riya', 10, ['20', '30']])\n",
      "Activity 4 output:  deque(['100', 'Ma', 'Baba', 'Sunny', 'Riya', 10, ['20', '30']])\n",
      "Output of Activity 5:  deque(['Ma', 'Baba', 'Sunny', 'Riya', 10])\n"
     ]
    }
   ],
   "source": [
    "# Initializing a deque object\n",
    "\n",
    "deq = deque([\"Sunny\",\"Riya\",10])\n",
    "\n",
    "print(\"Initial Version: \", deq)\n",
    "\n",
    "#Activity 2\n",
    "## Appending to the end of) the object\n",
    "deq.append([\"20\",\"30\"])\n",
    "print(\"After appending elements: \", deq)\n",
    "\n",
    "#Activity 3\n",
    "## Inserting multiple elements at the start of the deque object\n",
    "deq.extendleft([\"Baba\",\"Ma\"])\n",
    "print(deq)\n",
    "\n",
    "#Activity 4\n",
    "## Inserting element at the start of the deque object\n",
    "deq.appendleft(\"100\")\n",
    "print(\"Activity 4 output: \", deq)\n",
    "\n",
    "#Activity 5\n",
    "# Removing elements from the start and end of the deque object\n",
    "deq.pop() # removes element from end of deque object\n",
    "deq.popleft() #removes element from the start of deque object\n",
    "print(\"Output of Activity 5: \", deq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afffad1",
   "metadata": {},
   "source": [
    "## OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d6f4edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Dict:\n",
      "\n",
      "a 1\n",
      "b 2\n",
      "c 3\n",
      "d 4\n",
      "\n",
      "This is an Ordered Dict:\n",
      "\n",
      "a 1\n",
      "b 2\n",
      "c 3\n",
      "d 4\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict  \n",
    "    \n",
    "print(\"This is a Dict:\\n\")  \n",
    "d = {}  \n",
    "d['a'] = 1\n",
    "d['b'] = 2\n",
    "d['c'] = 3\n",
    "d['d'] = 4\n",
    "    \n",
    "for key, value in d.items():  \n",
    "    print(key, value)  \n",
    "    \n",
    "print(\"\\nThis is an Ordered Dict:\\n\")  \n",
    "od = OrderedDict()  \n",
    "od['a'] = 1\n",
    "od['b'] = 2\n",
    "od['c'] = 3\n",
    "od['d'] = 4\n",
    "    \n",
    "for key, value in od.items():  \n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f2ffa7",
   "metadata": {},
   "source": [
    "## DefaultDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2d6c19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('n', 4), ('s', 2), ('u', 2), ('y', 2)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "s = 'sunnyyunns'\n",
    "d = defaultdict(int)\n",
    "for k in s:\n",
    "    d[k] += 1\n",
    "\n",
    "sorted(d.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea7ada0",
   "metadata": {},
   "source": [
    "## UserString"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b78b9a",
   "metadata": {},
   "source": [
    "UserString is a string like container and just like UserDict and UserList it acts as a wrapper around string objects. It is used when someone wants to create their own strings with some modified or additional functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26334529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import UserString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "485e19ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity 1 output:  Sunnys\n",
      "Activity 2 output:  BabiSunnys\n"
     ]
    }
   ],
   "source": [
    "# Creating custom string methods to append letters at the start and end of a string\n",
    "\n",
    "class My_string(UserString):\n",
    "    # Method to append string \n",
    "    def append(self,s = \"\"):\n",
    "        self.data += s\n",
    "        \n",
    "    #Method to append at the start of the list\n",
    "    def append_left(self,s= \"\"):\n",
    "        self.data = s + self.data\n",
    "        \n",
    "st = My_string(\"Sunny\")\n",
    "\n",
    "#Activity 1\n",
    "st.append(\"s\")\n",
    "print(\"Activity 1 output: \", st)\n",
    "\n",
    "#Activity 2\n",
    "st.append_left(\"Babi\")\n",
    "print(\"Activity 2 output: \", st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "003505b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subho\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (5.1.0)/charset_normalizer (2.0.7) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Set up logging\n",
    "log_dir = 'logs'  # Create a folder named 'logs' to store log files\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "log_file = os.path.join(log_dir, 'training_log.txt')\n",
    "\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Your neural network model definition\n",
    "model = keras.models.Sequential([\n",
    "    # Add your layers here\n",
    "    # Example:\n",
    "    keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Load your training data\n",
    "# Example:\n",
    "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape((60000, 28 * 28)).astype('float32') / 255\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "\n",
    "# Train the model with logging\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logger.info(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    # Your training code here\n",
    "    # Example:\n",
    "    # model.fit(x_train, y_train, epochs=1, batch_size=32)\n",
    "\n",
    "    # Log relevant metrics\n",
    "    training_loss, training_accuracy = model.evaluate(x_train, y_train, verbose=0)\n",
    "    logger.info(f\"Training Loss: {training_loss:.4f}, Training Accuracy: {training_accuracy:.4f}\")\n",
    "\n",
    "    # Optionally, log additional metrics or information\n",
    "\n",
    "# End of training\n",
    "logger.info(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "084ed81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e892b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "\n",
    "def setup_logging(log_file):\n",
    "    logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logger = logging.getLogger()\n",
    "    return logger\n",
    "\n",
    "def train_neural_network(logger, input_size, hyperparameters, x_train, y_train, progress_bar):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Dense(hyperparameters['hidden_units'], activation='relu', input_shape=(input_size,)),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=hyperparameters['optimizer'], loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    epochs = hyperparameters['epochs']\n",
    "    for epoch in progress_bar(range(epochs), desc=f\"Set {progress_bar.desc}\"):\n",
    "        logger.info(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Your training code here\n",
    "        # Example:\n",
    "        model.fit(x_train, y_train, epochs=1, batch_size=32)\n",
    "\n",
    "        # Log relevant metrics\n",
    "        training_loss, training_accuracy = model.evaluate(x_train, y_train, verbose=0)\n",
    "        logger.info(f\"Training Loss: {training_loss:.4f}, Training Accuracy: {training_accuracy:.4f}\")\n",
    "\n",
    "        # Optionally, log additional metrics or information\n",
    "\n",
    "    logger.info(\"Training completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    log_dir = 'logs'\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    # Load your training data\n",
    "    # Example:\n",
    "    # (x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "    # x_train = x_train.reshape((60000, 28 * 28)).astype('float32') / 255\n",
    "    # y_train = keras.utils.to_categorical(y_train)\n",
    "\n",
    "    input_size = 784  # Update this with the appropriate input size for your data\n",
    "\n",
    "    hyperparameters_list = [\n",
    "        {'hidden_units': 128, 'optimizer': 'adam', 'epochs': 5},\n",
    "#         {'hidden_units': 256, 'optimizer': 'sgd', 'epochs': 5},\n",
    "        # Add more sets of hyperparameters as needed\n",
    "    ]\n",
    "\n",
    "    processes = []\n",
    "\n",
    "    for idx, hyperparameters in enumerate(hyperparameters_list):\n",
    "        log_file = os.path.join(log_dir, f'training_log_set{idx + 1}.txt')\n",
    "        logger = setup_logging(log_file)\n",
    "\n",
    "        # Create a progress bar for each set of hyperparameters\n",
    "        progress_bar = tqdm(total=hyperparameters['epochs'], position=idx, leave=False, desc=f\"Set {idx + 1}\")\n",
    "\n",
    "        # Create a process for each set of hyperparameters\n",
    "        process = multiprocessing.Process(\n",
    "            target=train_neural_network,\n",
    "            args=(logger, input_size, hyperparameters, x_train, y_train, progress_bar)\n",
    "        )\n",
    "        processes.append(process)\n",
    "\n",
    "    # Start all the processes\n",
    "    for process in processes:\n",
    "        process.start()\n",
    "\n",
    "    # Wait for all processes to finish\n",
    "    for process in processes:\n",
    "        process.join()\n",
    "\n",
    "    print(\"Training with multiprocessing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade3cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Set 1:   0%|                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "\n",
    "def setup_logging(log_file):\n",
    "    logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logger = logging.getLogger()\n",
    "    return logger\n",
    "\n",
    "def train_neural_network(logger, input_size, hyperparameters, x_train, y_train, progress_bar):\n",
    "    # Explicitly set start method to 'spawn'\n",
    "    multiprocessing.set_start_method('spawn', force=True)\n",
    "\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Dense(hyperparameters['hidden_units'], activation='relu', input_shape=(input_size,)),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=hyperparameters['optimizer'], loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    epochs = hyperparameters['epochs']\n",
    "    for epoch in progress_bar(range(epochs), desc=f\"Set {progress_bar.desc}\"):\n",
    "        logger.info(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Your training code here\n",
    "        # Example:\n",
    "        model.fit(x_train, y_train, epochs=1, batch_size=32)\n",
    "\n",
    "        # Log relevant metrics\n",
    "        training_loss, training_accuracy = model.evaluate(x_train, y_train, verbose=0)\n",
    "        logger.info(f\"Training Loss: {training_loss:.4f}, Training Accuracy: {training_accuracy:.4f}\")\n",
    "\n",
    "        # Optionally, log additional metrics or information\n",
    "\n",
    "    logger.info(\"Training completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    log_dir = 'logs'\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    # Load your training data\n",
    "    # Example:\n",
    "    # (x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "    # x_train = x_train.reshape((60000, 28 * 28)).astype('float32') / 255\n",
    "    # y_train = keras.utils.to_categorical(y_train)\n",
    "\n",
    "    input_size = 784  # Update this with the appropriate input size for your data\n",
    "\n",
    "    hyperparameters_list = [\n",
    "        {'hidden_units': 128, 'optimizer': 'adam', 'epochs': 5},\n",
    "        {'hidden_units': 256, 'optimizer': 'sgd', 'epochs': 5},\n",
    "        # Add more sets of hyperparameters as needed\n",
    "    ]\n",
    "\n",
    "    processes = []\n",
    "\n",
    "    for idx, hyperparameters in enumerate(hyperparameters_list):\n",
    "        log_file = os.path.join(log_dir, f'training_log_set{idx + 1}.txt')\n",
    "        logger = setup_logging(log_file)\n",
    "\n",
    "        # Create a progress bar for each set of hyperparameters\n",
    "        progress_bar = tqdm(total=hyperparameters['epochs'], position=idx, leave=False, desc=f\"Set {idx + 1}\")\n",
    "\n",
    "        # Create a process for each set of hyperparameters\n",
    "        process = multiprocessing.Process(\n",
    "            target=train_neural_network,\n",
    "            args=(logger, input_size, hyperparameters, x_train, y_train, progress_bar)\n",
    "        )\n",
    "        process.start()\n",
    "        processes.append(process)\n",
    "\n",
    "    # Start all the processes\n",
    "#     for process in processes:\n",
    "#         process.start()\n",
    "\n",
    "    # Wait for all processes to finish\n",
    "    for process in processes:\n",
    "        process.join()\n",
    "\n",
    "    print(\"Training with multiprocessing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c29a4890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n",
      "2\n",
      "[3, 4]\n",
      "2\n",
      "[5, 6]\n",
      "2\n",
      "[7, 8]\n",
      "2\n",
      "[9, 10]\n",
      "2\n",
      "Original list: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Result after multiprocessing addition: 0\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing \n",
    "\n",
    "# def perform_operation(numbers, result_queue):\n",
    "#     result = sum(numbers)\n",
    "#     result_queue.put(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample list of numbers\n",
    "    \n",
    "    def perform_operation(numbers, result_queue):\n",
    "        result = sum(numbers)\n",
    "        print(result)\n",
    "        result_queue.put(result)\n",
    "    \n",
    "    numbers_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "    # Number of processes to spawn\n",
    "    num_processes = 4\n",
    "\n",
    "    # Split the list into chunks for each process\n",
    "    chunk_size = len(numbers_list) // num_processes\n",
    "    chunks = [numbers_list[i:i + chunk_size] for i in range(0, len(numbers_list), chunk_size)]\n",
    "\n",
    "    # Create a multiprocessing.Queue to store results\n",
    "    result_queue = multiprocessing.Queue()\n",
    "\n",
    "    # Create and start processes\n",
    "    processes = []\n",
    "    for chunk in chunks:\n",
    "        print(chunk)\n",
    "        process = multiprocessing.Process(target=perform_operation, args=(chunk, result_queue),)\n",
    "        print(2)\n",
    "#         print(result_queue)\n",
    "        processes.append(process)\n",
    "        process.start()\n",
    "\n",
    "    # Wait for all processes to finish\n",
    "    for process in processes:\n",
    "        process.join()\n",
    "\n",
    "    # Collect results from the queue\n",
    "    results = []\n",
    "    while not result_queue.empty():\n",
    "        results.append(result_queue.get())\n",
    "\n",
    "    # Sum up the results\n",
    "    final_result = sum(results)\n",
    "\n",
    "    print(\"Original list:\", numbers_list)\n",
    "    print(\"Result after multiprocessing addition:\", final_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac06d005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "585f410e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef87e8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(name):\n",
    "    print('hello', name)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    p = multiprocessing.Process(target=f, args=('bob',))\n",
    "    p.start()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d852844f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
